# Research Notes: Ship with AI #1 — AI Makes You 17% Worse at Coding
**Date:** 2026-03-10 (publish date)
**Researched:** 2026-02-25
**Series:** Ship with AI (replaced Tue Tools)

## Manager Requirements
- Empty task description. No specific requirements.
- New series: Ship with AI = practical AI coding tips for engineering teams.
- Format guide: `reference/content-pillars/ai-coding-tips.md`.

## Source Scan (Feb 1-25, 2026)

### Primary Sources Consulted

| Source | What Found | Engagement |
|---|---|---|
| Hacker News | "How AI assistance impacts coding skills" discussion | 482 points, 347 comments |
| Reddit r/ExperiencedDevs | Multiple threads on AI skill degradation | High activity |
| Reddit r/programming | Anthropic study discussion | Trending |
| InfoQ | Coverage article on the study | Widely shared |
| DevClass | Coverage article on the study | Widely shared |

### Selected Topic: Anthropic AI Coding Skills Study

**Source:** Anthropic Research, published January 30, 2026
**URL:** https://www.anthropic.com/research/AI-assistance-coding-skills
**arXiv:** https://arxiv.org/abs/2601.20245
**HN Discussion:** https://news.ycombinator.com/item?id=46820924 (482 pts, 347 comments)

**Surface-level findings (widely covered):**
- Randomized controlled trial with n=52 professional developers (26 per group)
- AI coding assistance group scored 17% lower on skill mastery assessments vs. control (p=0.01, Cohen's d=0.738)
- Critical split: "delegation mode" (24-39% mastery) vs. "inquiry mode" (65-86% mastery)

**Deep findings (non-obvious, from actual paper):**
- **Speed benefit was NOT statistically significant.** Participants spent up to 11 minutes (30% of session) composing queries — the time saved by AI code was consumed by query composition time.
- **Senior devs (7+ years) showed the same learning degradation as juniors.** Experience provided zero protection. 29/52 participants had 7+ years experience — this is NOT a study about beginners.
- **Debugging was disproportionately destroyed.** 38.5% of AI users (10/26) finished with zero errors vs. 7.7% of control (2/26). The control group encountered 3x more errors (median 3 vs. median 1). Error prevention = learning prevention.
- **6 interaction patterns identified (not just 2):** AI Delegation (24-39%), Progressive AI Reliance, Iterative AI Debugging, Generation-Then-Comprehension (65%+), Hybrid Code-Explanation, Conceptual Inquiry (65-86%).
- **Generation-Then-Comprehension pattern:** Developers who pasted AI code then asked "why does this work?" scored 65%+ — ~2 extra minutes of follow-up questions flipped outcomes from bottom tier to top tier.
- **Iterative AI Debugging is deceptive:** These participants spent MORE time and asked MORE questions than delegators but learned just as little. "What's wrong with my code?" is not learning; "Why does structured concurrency work this way?" is.
- **Progressive AI Reliance showed within-session tipping point:** Participants started by asking questions, then degraded to full delegation within 30 minutes.
- **79 of 130 total AI queries were explanation requests** — yet the group STILL scored 17% lower overall. Asking "why" helps but doesn't fully close the gap vs. no AI at all.
- **100% of AI group completed both tasks vs. 84.6% of control.** Task completion was dramatically better with AI.
- **Anthropic tested GPT-4o, not their own model.** Adds credibility (no cherry-picking).
- **Pilot studies revealed 25-35% non-compliance** — control group participants kept finding ways to use AI even when told not to.
- **Quiz was administered immediately after coding** — long-term skill retention is unknown.

**Why selected:**
- Highest HN engagement of all candidates (482 points)
- From Anthropic itself — credibility + irony angle
- Universally relevant to every developer using AI tools
- Actionable fix (interaction patterns, not "stop using AI")
- Front-end relevant: component patterns, hooks, state management are exactly the things you stop learning deeply when delegating to AI

### Candidates Considered & Dropped

| Topic | Source Date | Why Dropped |
|---|---|---|
| Cursor Cloud Agents launch | Feb 24, 2026 | Too new for March 10 post, better fit for March 24 post paired with agentic engineering |
| Addy Osmani "Agentic Engineering" | Feb 4, 2026 | Saved for March 24 post — pairs better with Cursor Cloud Agents |
| Claude Code hooks/tips | Feb 3, 2026 | Too tool-specific for series debut, better as a later post |
| "Coding agents replaced every framework" (HN) | Feb ~7, 2026 | Provocative but not actionable enough for a tips post |

## Content Landscape

- The "AI makes devs worse" narrative is active on Reddit, HN, and tech media — but most coverage is doom-scrolling without a fix
- Our angle fills the gap: present the problem WITH the 3 interaction patterns that solve it
- No LinkedIn carousel-style content covering this study with actionable takeaways
- Strong debate potential: every developer has an opinion on whether AI helps or hurts skills

## Selection Rationale

- Highest engagement signal of all candidates (482 HN points)
- Peer-reviewed research adds credibility — not just opinion
- Irony angle (Anthropic researching harm from their own product) drives shares
- The fix (3 interaction patterns) makes it actionable, not just alarming
- Natural GFE tie-in: if AI weakens your fundamentals, deliberate practice (interview prep) is the antidote
- Perfect series debut: provocative, research-backed, universally relatable
