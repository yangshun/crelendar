# Research Notes: Ship with AI #2 — Stop Vibe Coding. Start Agentic Engineering.
**Date:** 2026-03-24 (publish date)
**Researched:** 2026-02-25, updated 2026-02-26
**Series:** Ship with AI (replaced Tue Tools)

## Manager Requirements
- Empty task description. No specific requirements.
- Recurring series: Ship with AI = practical AI coding tips for engineering teams.
- Format guide: `reference/content-pillars/ai-coding-tips.md`.

## Source Scan (Feb 1-25, 2026)

### Primary Sources Consulted

| Source | What Found | Engagement |
|---|---|---|
| Addy Osmani blog | "Agentic Engineering" post (Feb 4) | Widely shared |
| Addy Osmani Substack | "The 80% Problem in Agentic Coding" (Jan 28) | 296 likes, 54 comments, 43 shares |
| Addy Osmani Substack | "Code Review in the Age of AI" (Jan 5) | 145 likes, 12 comments, 19 shares |
| Hacker News | "Coding agents have replaced every framework I used" | 375 points, 594 comments |
| CNBC | Cursor Cloud Agents launch (Feb 24) | Major tech news |
| Salesforce Engineering | Two blog posts on Cursor adoption | Industry case study |
| O'Reilly Radar | "Conductors to Orchestrators" | Industry perspective |

### Selected Topic: Vibe Coding vs. Agentic Engineering

**Core framing:**
- "Vibe coding" coined by Andrej Karpathy (2025): fully giving in to AI suggestions, accepting every line, pasting errors back
- "Agentic engineering" coined by Addy Osmani (Feb 2026): orchestrating AI agents with human-owned architecture, specs, tests, and review gates
- The 70/30 rule (from "The 80% Problem" Substack, NOT the blog post): 70% of time on problem definition and verification, 30% on execution

**Key sources:**
- Osmani blog: https://addyosmani.com/blog/agentic-engineering/ (Feb 4, 2026)
- Osmani Substack: https://addyo.substack.com/p/the-80-problem-in-agentic-coding (Jan 28, 2026)
- Osmani Substack: https://addyo.substack.com/p/code-review-in-the-age-of-ai (Jan 5, 2026)
- HN thread: https://news.ycombinator.com/item?id=46923543 (375 pts, 594 comments)
- Cloudflare Vinext: https://blog.cloudflare.com/vinext/
- Salesforce Engineering: https://engineering.salesforce.com/how-salesforce-engineering-operationalized-ai-productivity-at-scale/
- Salesforce Engineering: https://engineering.salesforce.com/how-cursor-ai-cut-legacy-code-coverage-time-by-85/

## Deep Findings (from actual source material)

### Osmani "The 80% Problem" — the richest source

**Five failure modes of vibe coding:**
1. **Assumption Propagation** — model misunderstands something early and builds entire features on faulty premises. "You don't find out until you're five PRs deep and the architecture is cemented."
2. **Abstraction Bloat** — agents scaffold 1,000 lines where 100 suffice. "They're optimizing for looking comprehensive, not for maintainability."
3. **Dead Code Accumulation** — old implementations linger alongside new ones. Comments removed as side effects.
4. **Sycophantic Agreement** — "They don't always push back. No 'Are you sure?' or 'Have you considered...?'" Enthusiastic execution of incomplete descriptions.
5. **Conceptual Failures** — "AI errors evolved from syntax bugs to conceptual failures — the kind a sloppy, hasty junior may make under time pressure."

**Comprehension debt (Osmani's personal example):** "Claude implemented a feature I'd been putting off for days. Tests passed. I skimmed it, nodded, merged. Three days later I couldn't explain how it worked." The psychological hook: "You're always almost there. Just one more prompt." One developer reported: "that was 5 hrs ago" on a task that seemed "5 mins" away.

**The imperative → declarative shift:**
- Old (vibe coding): "Write a function that takes X and returns Y. Use this library. Handle these edge cases."
- New (agentic): "Here are requirements. Tests that must pass. Success criteria. Figure out how."
- Fresh-context reviews: give the same model a clean context window to critique its own output — catches its own mistakes.

**The bimodal split:** Two populations emerging, NOT a smooth curve. Early adopters: 100% AI code, dozens of PRs daily (Boris Cherny at Claude Code: "100% of our code is written by Claude Code + Opus 4.5"). Majority: still writing 90%+ manually.

### Key stats with FULL context

| Metric | Source | Detail |
|---|---|---|
| 98% more PRs merged | Faros AI / DORA 2025 | BUT: review time +91%, PR size +154%. Bottleneck shifted from writing to reviewing. |
| 91% longer review time | Same Faros AI / DORA 2025 | Per-PR, not total. Combined with volume spike = throughput crisis. |
| 99% save 10+ hours/week | Atlassian 2025 | BUT most report NO DECREASE in overall workload. Time saved consumed by organizational friction. |
| Only 16% "great" improvements | Stack Overflow 2025 | Top frustration: "almost right, but not quite" (66%), "debugging AI code takes longer than writing it" (45%). |
| Only 48% check AI code before committing | SonarSource survey | Over half merge unchecked AI output. |
| Logic errors at 1.75x | CodeRabbit report (NOT Anthropic) | Previously misattributed — corrected. |
| XSS vulnerabilities at 2.74x | Veracode study | ~45% of AI-generated code contains security flaws. |

### Cloudflare Vinext — deep details

- One engineer (engineering manager, not full-time coder) directed AI through 800+ sessions
- Used "OpenCode" platform with Claude API — NOT Cursor or Claude Code
- $1,100 total in Claude API tokens across all sessions
- Architecture defined in first hours: "I spent a couple of hours going back and forth with Claude in OpenCode to define the architecture" — served as the "north star" for all subsequent sessions
- Output: 1,700+ Vitest unit tests, 380 Playwright E2E tests, 94% API coverage of Next.js 16
- Build: 4.4x faster (1.67s vs. 7.38s), bundles 57% smaller (72.9 KB vs. 168.9 KB)
- Timeline: Feb 13 first commit → Feb 15 deploying to Cloudflare Workers. 3 days for core functionality.
- **Why it worked (non-obvious):** Next.js is "well-specified" with extensive docs/Stack Overflow (training data rich), has elaborate pre-existing test suites (verification pre-built), Vite provides solid foundation. "Even a few months earlier this would not have worked because earlier models couldn't sustain coherence across a codebase this size."
- **The surprising insight:** "It's not clear yet which abstractions are truly foundational and which ones were just crutches for human cognition."

### Salesforce Cursor adoption — the concrete details

- 90%+ developer adoption across all six engineering clouds (Data Cloud, Platform, Agentforce, MuleSoft, Tableau, Heroku)
- Achieved in 3 weeks, sustained for 6+ weeks
- 475 engineers exceeded default token limits
- Cut legacy code coverage time by 85%: from 26 engineer-days/module to 4. Generated 180,000 lines of test code in 12 days at 90-99% accuracy.
- **AI tests uncovered a production bug:** "revealed a health check logic error where an AND condition should have been an OR"
- **Failure modes:** Agent "went rogue, modifying hundreds of files at once." Cyclic loops requiring manual intervention. Required complete rollbacks initially. Had to shift from broad instructions to "one class at a time." Sonarqube validation required as final gate.

### HN thread — most insightful takes

- **The `agents.md` pattern:** Create a project-level file requiring AI to ask clarifying questions BEFORE coding, generate implementation plan for human review BEFORE execution. 5-15 minute execution cycles.
- **Domain-specific blindspot:** AI "fundamentally does not understand concurrency in Erlang/BEAM/OTP" despite appearing competent. Real-world GenServer implementations fail in practice.
- **Most credible productivity claim:** 1.5x on "drudge tasks" (migrations, edge case tests, CRUD, refactoring). NOT 5x on everything.
- **XKCD 1205 economics:** Tasks that were on the wrong side of the "is it worth your time" matrix are now trivial — "prompt and a code review."

## Source corrections from previous version

- ~~"75% more logic errors (Anthropic study)"~~ → Actually 1.75x logic error rate from CodeRabbit report, not Anthropic. Corrected in brief.
- The 70/30 rule is from "The 80% Problem" Substack article, not the "Agentic Engineering" blog post. Blog post is a positioning piece with minimal specifics.
- The "Agentic Engineering" blog post provides the TERM and THESIS but almost no implementation details. All concrete details come from the "80% Problem" and "Code Review" articles.

## Why selected

- Second-highest HN engagement (375 points, 594 comments)
- Osmani's credibility (Google Chrome team) gives the framework authority
- The "vibe coding vs. agentic engineering" framing is viscerally compelling
- Concrete case studies (Cloudflare, Salesforce) with both successes AND failures
- Complements Ship with AI #1 (which covered the problem; this covers the workflow solution)

### Candidates Considered & Dropped

| Topic | Source Date | Why Dropped |
|---|---|---|
| Anthropic AI skills study | Jan 30, 2026 | Already covered in Ship with AI #1 (March 10) |
| Claude Code advanced user tips | Feb 3, 2026 | Too tool-specific; save for a later post |
| "AI is killing my productivity" Reddit threads | Various Feb 2026 | Anecdotal; prefer the Osmani framework backed by data |

## Content Landscape

- "Vibe coding" is becoming a meme on developer Twitter/X — our post adds the professional counter-framework
- Multiple platforms picking up Osmani's "agentic engineering" framing (DEV Community, O'Reilly, Medium)
- LinkedIn gap: nobody has done a clean breakdown of the SPECIFIC failure modes with the SPECIFIC fixes and HONEST caveats on the proof
- Most coverage either hypes AI productivity or doom-scrolls about degradation — our post gives the nuanced middle with data

## Selection Rationale

- Fresh framing (Feb 2026): "agentic engineering" is the new vocabulary for this shift
- Osmani's credibility + real-world examples with BOTH successes and failures = trustworthy, not hype
- The five failure modes + imperative-to-declarative shift + `agents.md` pattern are all concrete and actionable
- The "98% more PRs / 91% longer reviews" paired stat is genuinely surprising and drives comments
- Natural GFE tie-in: agentic engineering "disproportionately benefits senior engineers" with strong fundamentals (Osmani's words)
